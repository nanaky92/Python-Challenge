{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the JSON\n",
    "\n",
    "source_data.json has clean normalized data used as the source of the matching.\n",
    "\n",
    "We need to load it and then we want to make it a pandas DataFrame, as it will make much easier all future data transaction. This is a fixed cost we must pay once for running all the functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'doctor': {'first_name': 'Dean',\n",
      "             'last_name': 'Israel',\n",
      "             'npi': '85103080143784778415'},\n",
      "  'practices': [{'city': 'Port Demetris',\n",
      "                 'lat': '-79.8757664338564',\n",
      "                 'lon': '84.31253504872467',\n",
      "                 'state': 'LA',\n",
      "                 'street': '271 Annabelle Fort',\n",
      "                 'street_2': 'Apt. 404',\n",
      "                 'zip': '53549'}]},\n",
      " {'doctor': {'first_name': 'Quinton',\n",
      "             'last_name': 'Mollie',\n",
      "             'npi': '36233383542350521233'},\n",
      "  'practices': [{'city': 'Nealville',\n",
      "                 'lat': '81.37417480720865',\n",
      "                 'lon': '-95.33450729432164',\n",
      "                 'state': 'OR',\n",
      "                 'street': '8496 Kennedi Inlet',\n",
      "                 'street_2': 'Suite 815',\n",
      "                 'zip': '52665-6811'},\n",
      "                {'city': 'Rashadborough',\n",
      "                 'lat': '69.84837521604314',\n",
      "                 'lon': '87.36942972635728',\n",
      "                 'state': 'UT',\n",
      "                 'street': '29483 Nader Wall',\n",
      "                 'street_2': 'Apt. 748',\n",
      "                 'zip': '46006-3437'},\n",
      "                {'city': 'South Daronland',\n",
      "                 'lat': '84.90377842497296',\n",
      "                 'lon': '177.28706015725533',\n",
      "                 'state': 'AK',\n",
      "                 'street': '2122 Wintheiser Valleys',\n",
      "                 'street_2': 'Suite 855',\n",
      "                 'zip': '99372'}]}]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "def extract_source_data(source_file):\n",
    "    ''' Form a list of dictionaries from a file with a json doc per line''' \n",
    "    source_data = []\n",
    "    with open(source_file) as f:\n",
    "        for line in f:\n",
    "            source_data.append(json.loads(line))\n",
    "    return source_data\n",
    "\n",
    "source_data = extract_source_data(\"source_data.json\")\n",
    "pprint(source_data[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unwinding the json\n",
    "\n",
    "We want to unwind the practices in order to:\n",
    "\n",
    "- Look for number of doctors from match_file.csv in which first name, last name and full adress match with the source_data.\n",
    "- Look for number of practices from match_file.csv in which the full adress match with the source_data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>state</th>\n",
       "      <th>street</th>\n",
       "      <th>street_2</th>\n",
       "      <th>zip</th>\n",
       "      <th>first_name</th>\n",
       "      <th>last_name</th>\n",
       "      <th>npi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Port Demetris</td>\n",
       "      <td>-79.8757664338564</td>\n",
       "      <td>84.31253504872467</td>\n",
       "      <td>LA</td>\n",
       "      <td>271 Annabelle Fort</td>\n",
       "      <td>Apt. 404</td>\n",
       "      <td>53549</td>\n",
       "      <td>Dean</td>\n",
       "      <td>Israel</td>\n",
       "      <td>85103080143784778415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nealville</td>\n",
       "      <td>81.37417480720865</td>\n",
       "      <td>-95.33450729432164</td>\n",
       "      <td>OR</td>\n",
       "      <td>8496 Kennedi Inlet</td>\n",
       "      <td>Suite 815</td>\n",
       "      <td>52665-6811</td>\n",
       "      <td>Quinton</td>\n",
       "      <td>Mollie</td>\n",
       "      <td>36233383542350521233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Rashadborough</td>\n",
       "      <td>69.84837521604314</td>\n",
       "      <td>87.36942972635728</td>\n",
       "      <td>UT</td>\n",
       "      <td>29483 Nader Wall</td>\n",
       "      <td>Apt. 748</td>\n",
       "      <td>46006-3437</td>\n",
       "      <td>Quinton</td>\n",
       "      <td>Mollie</td>\n",
       "      <td>36233383542350521233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>South Daronland</td>\n",
       "      <td>84.90377842497296</td>\n",
       "      <td>177.28706015725533</td>\n",
       "      <td>AK</td>\n",
       "      <td>2122 Wintheiser Valleys</td>\n",
       "      <td>Suite 855</td>\n",
       "      <td>99372</td>\n",
       "      <td>Quinton</td>\n",
       "      <td>Mollie</td>\n",
       "      <td>36233383542350521233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>West Lonnieberg</td>\n",
       "      <td>52.12502086274685</td>\n",
       "      <td>109.12414094328233</td>\n",
       "      <td>GA</td>\n",
       "      <td>210 Walsh Island</td>\n",
       "      <td>Suite 839</td>\n",
       "      <td>59104</td>\n",
       "      <td>Vincent</td>\n",
       "      <td>Abbie</td>\n",
       "      <td>68951826121607537145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Port Angieborough</td>\n",
       "      <td>89.41473074638557</td>\n",
       "      <td>-38.22151510102702</td>\n",
       "      <td>KY</td>\n",
       "      <td>460 Ortiz Points</td>\n",
       "      <td>Suite 609</td>\n",
       "      <td>60776-9928</td>\n",
       "      <td>Vincent</td>\n",
       "      <td>Abbie</td>\n",
       "      <td>68951826121607537145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Nyasiaburgh</td>\n",
       "      <td>0.7514069044332956</td>\n",
       "      <td>93.56993517086102</td>\n",
       "      <td>NH</td>\n",
       "      <td>13810 Pfannerstill Pike</td>\n",
       "      <td>Apt. 165</td>\n",
       "      <td>71167-1710</td>\n",
       "      <td>Vincent</td>\n",
       "      <td>Abbie</td>\n",
       "      <td>68951826121607537145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Grantborough</td>\n",
       "      <td>78.53231427000821</td>\n",
       "      <td>12.229188372184922</td>\n",
       "      <td>MN</td>\n",
       "      <td>1262 O'Keefe Ford</td>\n",
       "      <td>Apt. 790</td>\n",
       "      <td>39283</td>\n",
       "      <td>Gerardo</td>\n",
       "      <td>Piper</td>\n",
       "      <td>92442805782715742535</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                city                 lat                 lon state  \\\n",
       "0      Port Demetris   -79.8757664338564   84.31253504872467    LA   \n",
       "1          Nealville   81.37417480720865  -95.33450729432164    OR   \n",
       "2      Rashadborough   69.84837521604314   87.36942972635728    UT   \n",
       "3    South Daronland   84.90377842497296  177.28706015725533    AK   \n",
       "4    West Lonnieberg   52.12502086274685  109.12414094328233    GA   \n",
       "5  Port Angieborough   89.41473074638557  -38.22151510102702    KY   \n",
       "6        Nyasiaburgh  0.7514069044332956   93.56993517086102    NH   \n",
       "7       Grantborough   78.53231427000821  12.229188372184922    MN   \n",
       "\n",
       "                    street   street_2         zip first_name last_name  \\\n",
       "0       271 Annabelle Fort   Apt. 404       53549       Dean    Israel   \n",
       "1       8496 Kennedi Inlet  Suite 815  52665-6811    Quinton    Mollie   \n",
       "2         29483 Nader Wall   Apt. 748  46006-3437    Quinton    Mollie   \n",
       "3  2122 Wintheiser Valleys  Suite 855       99372    Quinton    Mollie   \n",
       "4         210 Walsh Island  Suite 839       59104    Vincent     Abbie   \n",
       "5         460 Ortiz Points  Suite 609  60776-9928    Vincent     Abbie   \n",
       "6  13810 Pfannerstill Pike   Apt. 165  71167-1710    Vincent     Abbie   \n",
       "7        1262 O'Keefe Ford   Apt. 790       39283    Gerardo     Piper   \n",
       "\n",
       "                    npi  \n",
       "0  85103080143784778415  \n",
       "1  36233383542350521233  \n",
       "2  36233383542350521233  \n",
       "3  36233383542350521233  \n",
       "4  68951826121607537145  \n",
       "5  68951826121607537145  \n",
       "6  68951826121607537145  \n",
       "7  92442805782715742535  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize\n",
    "\n",
    "def transform_source_data_unwinding_fields(source_data):\n",
    "    '''Form a DataFrame from a list of dictionaries. \n",
    "    Rename columns to follow name conventions as the csv'''\n",
    "    fields_to_unwind = [\"practices\"]\n",
    "    not_unwinded_fields_path = [[\"doctor\", \"first_name\"], \n",
    "                           [\"doctor\", \"last_name\"], \n",
    "                           [\"doctor\", \"npi\"]]\n",
    "    not_unwinded_naming_map = {'doctor.first_name':'first_name', \n",
    "                               'doctor.last_name':'last_name',\n",
    "                               'doctor.npi':'npi'}\n",
    "    return json_normalize(source_data, \n",
    "                         fields_to_unwind, \n",
    "                         not_unwinded_fields_path).rename(\n",
    "                             columns = not_unwinded_naming_map)\n",
    "\n",
    "source_unwinded_df = transform_source_data_unwinding_fields(source_data)\n",
    "source_unwinded_df.head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also find a slightly more complicated version that is more generic, allowing us specify several fields at once to unwind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>state</th>\n",
       "      <th>street</th>\n",
       "      <th>street_2</th>\n",
       "      <th>zip</th>\n",
       "      <th>first_name</th>\n",
       "      <th>last_name</th>\n",
       "      <th>npi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Port Demetris</td>\n",
       "      <td>-79.8757664338564</td>\n",
       "      <td>84.31253504872467</td>\n",
       "      <td>LA</td>\n",
       "      <td>271 Annabelle Fort</td>\n",
       "      <td>Apt. 404</td>\n",
       "      <td>53549</td>\n",
       "      <td>Dean</td>\n",
       "      <td>Israel</td>\n",
       "      <td>85103080143784778415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nealville</td>\n",
       "      <td>81.37417480720865</td>\n",
       "      <td>-95.33450729432164</td>\n",
       "      <td>OR</td>\n",
       "      <td>8496 Kennedi Inlet</td>\n",
       "      <td>Suite 815</td>\n",
       "      <td>52665-6811</td>\n",
       "      <td>Quinton</td>\n",
       "      <td>Mollie</td>\n",
       "      <td>36233383542350521233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Rashadborough</td>\n",
       "      <td>69.84837521604314</td>\n",
       "      <td>87.36942972635728</td>\n",
       "      <td>UT</td>\n",
       "      <td>29483 Nader Wall</td>\n",
       "      <td>Apt. 748</td>\n",
       "      <td>46006-3437</td>\n",
       "      <td>Quinton</td>\n",
       "      <td>Mollie</td>\n",
       "      <td>36233383542350521233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>South Daronland</td>\n",
       "      <td>84.90377842497296</td>\n",
       "      <td>177.28706015725533</td>\n",
       "      <td>AK</td>\n",
       "      <td>2122 Wintheiser Valleys</td>\n",
       "      <td>Suite 855</td>\n",
       "      <td>99372</td>\n",
       "      <td>Quinton</td>\n",
       "      <td>Mollie</td>\n",
       "      <td>36233383542350521233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>West Lonnieberg</td>\n",
       "      <td>52.12502086274685</td>\n",
       "      <td>109.12414094328233</td>\n",
       "      <td>GA</td>\n",
       "      <td>210 Walsh Island</td>\n",
       "      <td>Suite 839</td>\n",
       "      <td>59104</td>\n",
       "      <td>Vincent</td>\n",
       "      <td>Abbie</td>\n",
       "      <td>68951826121607537145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Port Angieborough</td>\n",
       "      <td>89.41473074638557</td>\n",
       "      <td>-38.22151510102702</td>\n",
       "      <td>KY</td>\n",
       "      <td>460 Ortiz Points</td>\n",
       "      <td>Suite 609</td>\n",
       "      <td>60776-9928</td>\n",
       "      <td>Vincent</td>\n",
       "      <td>Abbie</td>\n",
       "      <td>68951826121607537145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Nyasiaburgh</td>\n",
       "      <td>0.7514069044332956</td>\n",
       "      <td>93.56993517086102</td>\n",
       "      <td>NH</td>\n",
       "      <td>13810 Pfannerstill Pike</td>\n",
       "      <td>Apt. 165</td>\n",
       "      <td>71167-1710</td>\n",
       "      <td>Vincent</td>\n",
       "      <td>Abbie</td>\n",
       "      <td>68951826121607537145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Grantborough</td>\n",
       "      <td>78.53231427000821</td>\n",
       "      <td>12.229188372184922</td>\n",
       "      <td>MN</td>\n",
       "      <td>1262 O'Keefe Ford</td>\n",
       "      <td>Apt. 790</td>\n",
       "      <td>39283</td>\n",
       "      <td>Gerardo</td>\n",
       "      <td>Piper</td>\n",
       "      <td>92442805782715742535</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                city                 lat                 lon state  \\\n",
       "0      Port Demetris   -79.8757664338564   84.31253504872467    LA   \n",
       "1          Nealville   81.37417480720865  -95.33450729432164    OR   \n",
       "2      Rashadborough   69.84837521604314   87.36942972635728    UT   \n",
       "3    South Daronland   84.90377842497296  177.28706015725533    AK   \n",
       "4    West Lonnieberg   52.12502086274685  109.12414094328233    GA   \n",
       "5  Port Angieborough   89.41473074638557  -38.22151510102702    KY   \n",
       "6        Nyasiaburgh  0.7514069044332956   93.56993517086102    NH   \n",
       "7       Grantborough   78.53231427000821  12.229188372184922    MN   \n",
       "\n",
       "                    street   street_2         zip first_name last_name  \\\n",
       "0       271 Annabelle Fort   Apt. 404       53549       Dean    Israel   \n",
       "1       8496 Kennedi Inlet  Suite 815  52665-6811    Quinton    Mollie   \n",
       "2         29483 Nader Wall   Apt. 748  46006-3437    Quinton    Mollie   \n",
       "3  2122 Wintheiser Valleys  Suite 855       99372    Quinton    Mollie   \n",
       "4         210 Walsh Island  Suite 839       59104    Vincent     Abbie   \n",
       "5         460 Ortiz Points  Suite 609  60776-9928    Vincent     Abbie   \n",
       "6  13810 Pfannerstill Pike   Apt. 165  71167-1710    Vincent     Abbie   \n",
       "7        1262 O'Keefe Ford   Apt. 790       39283    Gerardo     Piper   \n",
       "\n",
       "                    npi  \n",
       "0  85103080143784778415  \n",
       "1  36233383542350521233  \n",
       "2  36233383542350521233  \n",
       "3  36233383542350521233  \n",
       "4  68951826121607537145  \n",
       "5  68951826121607537145  \n",
       "6  68951826121607537145  \n",
       "7  92442805782715742535  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize\n",
    "\n",
    "def get_formatted_not_unwinded_fields(source_series, fields_not_to_unwind):\n",
    "    '''Form a list of lists with the path to the fields inside fields_to_unwind,\n",
    "    and a dict mapping this path to the subfields \n",
    "    \n",
    "    These are needed in order to use the json_normalize in pandas.io.json.\n",
    "    \n",
    "    Example: {\"doctor\": {\"last_name\": \"Doe\", \"first_name\": \"John\"}} will return\n",
    "    -not_unwinded_fields_path = [[\"doctor\", \"first_name\"], \n",
    "                                [\"doctor\", \"last_name\"]]\n",
    "    -not_unwinded_naming_map = {\"doctor.first_name\": \"first_name\", \n",
    "                                \"doctor.last_name\": \"last_name\"}'''\n",
    "    not_unwinded_fields_path = []\n",
    "    not_unwinded_naming_map = {}\n",
    "    for field in source_series:\n",
    "        if field in fields_not_to_unwind:\n",
    "            for subfield in source_series[field]:\n",
    "                not_unwinded_fields_path.append([field, subfield])\n",
    "                not_unwinded_naming_map[\"{}.{}\".format(field, subfield)] = subfield\n",
    "    return not_unwinded_fields_path, not_unwinded_naming_map\n",
    "\n",
    "def transform_source_data_unwinding_fields(source_data, fields_to_unwind, fields_not_to_unwind):\n",
    "    '''Form a DataFrame from a list of dictionaries. \n",
    "    Rename columns to follow name conventions as the csv'''\n",
    "    if len(source_data) != 0:\n",
    "        not_unwinded_fields_path, not_unwinded_naming_map = \\\n",
    "            get_formatted_not_unwinded_fields(source_data[0], fields_not_to_unwind)\n",
    "    else:\n",
    "        return None\n",
    "    return json_normalize(source_data, \n",
    "                         fields_to_unwind, \n",
    "                         not_unwinded_fields_path).rename(\n",
    "                             columns = not_unwinded_naming_map)\n",
    "\n",
    "source_unwinded_df = transform_source_data_unwinding_fields(source_data, \n",
    "                                                           fields_to_unwind=[\"practices\"], \n",
    "                                                           fields_not_to_unwind=[\"doctor\"])\n",
    "source_unwinded_df.head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without unwinding\n",
    "\n",
    "In case we want to look for number of doctors from match_file.csv in which the npi matches with the source_data, we canot have the unwinding in place for practices.\n",
    "\n",
    "If we use the unwinded dataframe and N is the length of the list of practices for a doctor where we match the npi, we would have N matches. \n",
    "\n",
    "In other usecases not part of the assignment, we would need to be careful when using the unwinded version to not repeat the doctors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first_name</th>\n",
       "      <th>last_name</th>\n",
       "      <th>npi</th>\n",
       "      <th>practices</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dean</td>\n",
       "      <td>Israel</td>\n",
       "      <td>85103080143784778415</td>\n",
       "      <td>[{'city': 'Port Demetris', 'street_2': 'Apt. 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quinton</td>\n",
       "      <td>Mollie</td>\n",
       "      <td>36233383542350521233</td>\n",
       "      <td>[{'city': 'Nealville', 'street_2': 'Suite 815'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Vincent</td>\n",
       "      <td>Abbie</td>\n",
       "      <td>68951826121607537145</td>\n",
       "      <td>[{'city': 'West Lonnieberg', 'street_2': 'Suit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gerardo</td>\n",
       "      <td>Piper</td>\n",
       "      <td>92442805782715742535</td>\n",
       "      <td>[{'city': 'Grantborough', 'street_2': 'Apt. 79...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dean</td>\n",
       "      <td>Francesco</td>\n",
       "      <td>83029151715578341587</td>\n",
       "      <td>[{'city': 'New Fredy', 'street_2': 'Suite 356'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Marshall</td>\n",
       "      <td>Cole</td>\n",
       "      <td>18233577393219566041</td>\n",
       "      <td>[{'city': 'Lake Sheila', 'street_2': 'Apt. 314...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Lawson</td>\n",
       "      <td>Lilliana</td>\n",
       "      <td>78792788275411915642</td>\n",
       "      <td>[{'city': 'North Daija', 'street_2': 'Apt. 256...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Coty</td>\n",
       "      <td>Brad</td>\n",
       "      <td>50391514247237749255</td>\n",
       "      <td>[{'city': 'West Calistaside', 'street_2': 'Apt...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  first_name  last_name                   npi  \\\n",
       "0       Dean     Israel  85103080143784778415   \n",
       "1    Quinton     Mollie  36233383542350521233   \n",
       "2    Vincent      Abbie  68951826121607537145   \n",
       "3    Gerardo      Piper  92442805782715742535   \n",
       "4       Dean  Francesco  83029151715578341587   \n",
       "5   Marshall       Cole  18233577393219566041   \n",
       "6     Lawson   Lilliana  78792788275411915642   \n",
       "7       Coty       Brad  50391514247237749255   \n",
       "\n",
       "                                           practices  \n",
       "0  [{'city': 'Port Demetris', 'street_2': 'Apt. 4...  \n",
       "1  [{'city': 'Nealville', 'street_2': 'Suite 815'...  \n",
       "2  [{'city': 'West Lonnieberg', 'street_2': 'Suit...  \n",
       "3  [{'city': 'Grantborough', 'street_2': 'Apt. 79...  \n",
       "4  [{'city': 'New Fredy', 'street_2': 'Suite 356'...  \n",
       "5  [{'city': 'Lake Sheila', 'street_2': 'Apt. 314...  \n",
       "6  [{'city': 'North Daija', 'street_2': 'Apt. 256...  \n",
       "7  [{'city': 'West Calistaside', 'street_2': 'Apt...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def transform_source_data_without_unwinding(source_data, fields_not_to_unwind):\n",
    "    '''Form a DataFrame from a list of dictionaries. \n",
    "    Rename columns to follow name conventions as the csv'''\n",
    "    if len(source_data) != 0:\n",
    "        not_unwinded_naming_map = \\\n",
    "            get_formatted_not_unwinded_fields(source_data[0], fields_not_to_unwind)[1]\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    return json_normalize(source_data).rename(\n",
    "        columns = not_unwinded_naming_map)\n",
    "\n",
    "source_not_unwinded_df = transform_source_data_without_unwinding(source_data, [\"doctor\"])\n",
    "source_not_unwinded_df.head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the csv\n",
    "\n",
    "match_file.csv contains raw source data that needs to be parsed and normalized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first_name</th>\n",
       "      <th>last_name</th>\n",
       "      <th>npi</th>\n",
       "      <th>street</th>\n",
       "      <th>street_2</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>zip</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ruthe</td>\n",
       "      <td>Laverne</td>\n",
       "      <td>44843147983186317848</td>\n",
       "      <td>569 glenda islands</td>\n",
       "      <td>suite 163</td>\n",
       "      <td>willport</td>\n",
       "      <td>nj</td>\n",
       "      <td>23453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Marshall</td>\n",
       "      <td>Cole</td>\n",
       "      <td>18233577393219566041</td>\n",
       "      <td>59944 adaline harbor</td>\n",
       "      <td>apt. 862</td>\n",
       "      <td>keelingstad</td>\n",
       "      <td>al</td>\n",
       "      <td>94189-5965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lawson</td>\n",
       "      <td>Lilliana</td>\n",
       "      <td>78792788275411915642</td>\n",
       "      <td>36175 amina mount</td>\n",
       "      <td>apt. 256</td>\n",
       "      <td>north daija</td>\n",
       "      <td>de</td>\n",
       "      <td>30997-4476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Martine</td>\n",
       "      <td>Kiana</td>\n",
       "      <td>23583155472740817761</td>\n",
       "      <td>188 walsh flat</td>\n",
       "      <td>apt. 891</td>\n",
       "      <td>yasmeenstad</td>\n",
       "      <td>nv</td>\n",
       "      <td>83568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Leatha</td>\n",
       "      <td>Freida</td>\n",
       "      <td>NaN</td>\n",
       "      <td>43796 gutmann plains</td>\n",
       "      <td>suite 341</td>\n",
       "      <td>vonmouth</td>\n",
       "      <td>fl</td>\n",
       "      <td>10500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Justyn</td>\n",
       "      <td>Abbie</td>\n",
       "      <td>78362387662864903554</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Granville</td>\n",
       "      <td>Benton</td>\n",
       "      <td>17871640342222098849</td>\n",
       "      <td>95496 dare rue</td>\n",
       "      <td>suite 203</td>\n",
       "      <td>octaviastad</td>\n",
       "      <td>il</td>\n",
       "      <td>45294-0751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Brenda</td>\n",
       "      <td>Lenna</td>\n",
       "      <td>88137148807320232511</td>\n",
       "      <td>361 justyn meadow</td>\n",
       "      <td>suite 635</td>\n",
       "      <td>steuberhaven</td>\n",
       "      <td>la</td>\n",
       "      <td>71148-1931</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  first_name last_name                   npi                street   street_2  \\\n",
       "0      Ruthe   Laverne  44843147983186317848    569 glenda islands  suite 163   \n",
       "1   Marshall      Cole  18233577393219566041  59944 adaline harbor   apt. 862   \n",
       "2     Lawson  Lilliana  78792788275411915642     36175 amina mount   apt. 256   \n",
       "3    Martine     Kiana  23583155472740817761        188 walsh flat   apt. 891   \n",
       "4     Leatha    Freida                   NaN  43796 gutmann plains  suite 341   \n",
       "5     Justyn     Abbie  78362387662864903554                   NaN        NaN   \n",
       "6  Granville    Benton  17871640342222098849        95496 dare rue  suite 203   \n",
       "7     Brenda     Lenna  88137148807320232511     361 justyn meadow  suite 635   \n",
       "\n",
       "           city state         zip  \n",
       "0      willport    nj       23453  \n",
       "1   keelingstad    al  94189-5965  \n",
       "2   north daija    de  30997-4476  \n",
       "3   yasmeenstad    nv       83568  \n",
       "4      vonmouth    fl       10500  \n",
       "5           NaN   NaN         NaN  \n",
       "6   octaviastad    il  45294-0751  \n",
       "7  steuberhaven    la  71148-1931  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# match_file.csv: Raw source data that needs to be parsed and normalized.\n",
    "raw_data_df = pd.read_csv(\"match_file.csv\")\n",
    "\n",
    "raw_data_df.head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are values that are NaN, so we need to be careful.\n",
    "\n",
    "In addition, some string fields don't have the appropriate capitalization. Let's fix that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first_name</th>\n",
       "      <th>last_name</th>\n",
       "      <th>npi</th>\n",
       "      <th>street</th>\n",
       "      <th>street_2</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>zip</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ruthe</td>\n",
       "      <td>Laverne</td>\n",
       "      <td>44843147983186317848</td>\n",
       "      <td>569 Glenda Islands</td>\n",
       "      <td>Suite 163</td>\n",
       "      <td>Willport</td>\n",
       "      <td>NJ</td>\n",
       "      <td>23453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Marshall</td>\n",
       "      <td>Cole</td>\n",
       "      <td>18233577393219566041</td>\n",
       "      <td>59944 Adaline Harbor</td>\n",
       "      <td>Apt. 862</td>\n",
       "      <td>Keelingstad</td>\n",
       "      <td>AL</td>\n",
       "      <td>94189-5965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lawson</td>\n",
       "      <td>Lilliana</td>\n",
       "      <td>78792788275411915642</td>\n",
       "      <td>36175 Amina Mount</td>\n",
       "      <td>Apt. 256</td>\n",
       "      <td>North Daija</td>\n",
       "      <td>DE</td>\n",
       "      <td>30997-4476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Martine</td>\n",
       "      <td>Kiana</td>\n",
       "      <td>23583155472740817761</td>\n",
       "      <td>188 Walsh Flat</td>\n",
       "      <td>Apt. 891</td>\n",
       "      <td>Yasmeenstad</td>\n",
       "      <td>NV</td>\n",
       "      <td>83568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Leatha</td>\n",
       "      <td>Freida</td>\n",
       "      <td>NaN</td>\n",
       "      <td>43796 Gutmann Plains</td>\n",
       "      <td>Suite 341</td>\n",
       "      <td>Vonmouth</td>\n",
       "      <td>FL</td>\n",
       "      <td>10500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Justyn</td>\n",
       "      <td>Abbie</td>\n",
       "      <td>78362387662864903554</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Granville</td>\n",
       "      <td>Benton</td>\n",
       "      <td>17871640342222098849</td>\n",
       "      <td>95496 Dare Rue</td>\n",
       "      <td>Suite 203</td>\n",
       "      <td>Octaviastad</td>\n",
       "      <td>IL</td>\n",
       "      <td>45294-0751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Brenda</td>\n",
       "      <td>Lenna</td>\n",
       "      <td>88137148807320232511</td>\n",
       "      <td>361 Justyn Meadow</td>\n",
       "      <td>Suite 635</td>\n",
       "      <td>Steuberhaven</td>\n",
       "      <td>LA</td>\n",
       "      <td>71148-1931</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  first_name last_name                   npi                street   street_2  \\\n",
       "0      Ruthe   Laverne  44843147983186317848    569 Glenda Islands  Suite 163   \n",
       "1   Marshall      Cole  18233577393219566041  59944 Adaline Harbor   Apt. 862   \n",
       "2     Lawson  Lilliana  78792788275411915642     36175 Amina Mount   Apt. 256   \n",
       "3    Martine     Kiana  23583155472740817761        188 Walsh Flat   Apt. 891   \n",
       "4     Leatha    Freida                   NaN  43796 Gutmann Plains  Suite 341   \n",
       "5     Justyn     Abbie  78362387662864903554                   NaN        NaN   \n",
       "6  Granville    Benton  17871640342222098849        95496 Dare Rue  Suite 203   \n",
       "7     Brenda     Lenna  88137148807320232511     361 Justyn Meadow  Suite 635   \n",
       "\n",
       "           city state         zip  \n",
       "0      Willport    NJ       23453  \n",
       "1   Keelingstad    AL  94189-5965  \n",
       "2   North Daija    DE  30997-4476  \n",
       "3   Yasmeenstad    NV       83568  \n",
       "4      Vonmouth    FL       10500  \n",
       "5           NaN   NaN         NaN  \n",
       "6   Octaviastad    IL  45294-0751  \n",
       "7  Steuberhaven    LA  71148-1931  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def transform_match_file(df, fields_to_title_case, fields_to_upper_case):\n",
    "    '''Change strings of certain DataFrame columns to title case and upper case'''\n",
    "    for field in fields_to_upper_case:\n",
    "        df[field] = df[field].apply(lambda x: x.title() if isinstance(x, str) else x)\n",
    "        \n",
    "    for field in fields_to_title_case:\n",
    "        df[field] = df[field].apply(lambda x: x.upper() if isinstance(x, str) else x)    \n",
    "\n",
    "\n",
    "fields_to_title_case = [\"state\"]\n",
    "fields_to_upper_case = [\"street\", \"street_2\", \"city\"]\n",
    "\n",
    "transform_match_file(raw_data_df, fields_to_title_case, fields_to_upper_case)\n",
    "\n",
    "raw_data_df.head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RESULTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doctor Match by NPI\n",
    "\n",
    "We do an inner join on the npis from the two dataframes, getting only the docs that have the same npis.\n",
    "\n",
    "The merge is O(M+N), if M is the number of rows in the json and N is the number of rows in the csv. \n",
    "\n",
    "Therefore, the total number of documents scanned is 2 * O(N+M) (so O(N+M)) \n",
    "\n",
    "- I scan everything for creating the DataFrames O(N+M)\n",
    "- I scan O(N+M) documents for doing the merging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of matches: 864\n"
     ]
    }
   ],
   "source": [
    "def number_of_doctor_matches_by_npi(left_df, right_df):\n",
    "    '''Return the number of doctor matches by npi.\n",
    "    This is done through the inner merge of two dataframes on the key npi.\n",
    "    The number of rows is the number of matches'''\n",
    "    return pd.merge(left_df, right_df, how=\"inner\", on=\"npi\").shape[0]\n",
    "    \n",
    "print(\"Number of matches: {}\".format(\n",
    "    number_of_doctor_matches_by_npi(source_not_unwinded_df, raw_data_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run number_of_doctor_matches_by_npi once 0.005226856389999739\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "\n",
    "time_to_run_100 = timeit.timeit(\n",
    "    'number_of_doctor_matches_by_npi(source_not_unwinded_df, raw_data_df)', \n",
    "    setup=\"from __main__ import number_of_doctor_matches_by_npi, source_not_unwinded_df, raw_data_df\", \n",
    "    number=100)\n",
    "\n",
    "print(\"Time to run number_of_doctor_matches_by_npi once {}\".format(time_to_run_100/100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to take into account that we are also transforming the data in order to do this that fast (although we only need to it once for similar operations). This operation is much, much slower:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to transform the data without unwinding once 0.6310522743200272\n"
     ]
    }
   ],
   "source": [
    "time_to_run_100 = timeit.timeit(\n",
    "    'transform_source_data_without_unwinding(source_data, [\"practices\"])', \n",
    "    setup=\"from __main__ import transform_source_data_without_unwinding, source_data\", \n",
    "    number=100)\n",
    "\n",
    "print(\"Time to transform the data without unwinding once {}\".format(time_to_run_100/100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If our only intereset was this part of the assignment, we don't need to do the processing of the json. If we extract the npis in the json and use a set to store them, the resulting function is 2 orders of magnitude faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "864"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def number_of_matches_by_npi(source_data, raw_data_df):\n",
    "    '''Return the number of doctor matches by npi.\n",
    "    -First extract all the npis from the json, and \n",
    "    insert them in a test for fast lookup (O(1) on average).\n",
    "    -Sum the number of elements in the raw df whose npi is in the set'''\n",
    "    source_npis = {x[\"doctor\"][\"npi\"] for x in source_data}\n",
    "    return raw_data_df[\"npi\"].apply(lambda x: x in source_npis).sum()\n",
    "\n",
    "number_of_matches_by_npi(source_data, raw_data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run number_of_matches_by_npi (set implementation) once 0.007534609550020832\n"
     ]
    }
   ],
   "source": [
    "time_to_run_100 = timeit.timeit(\n",
    "    'number_of_matches_by_npi(source_data, raw_data_df)', \n",
    "    setup=\"from __main__ import number_of_matches_by_npi, source_data, raw_data_df\", \n",
    "    number=100)\n",
    "\n",
    "print(\"Time to run number_of_matches_by_npi (set implementation) once {}\".format(\n",
    "    time_to_run_100/100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doctor Match by first name, last name and full adress\n",
    "\n",
    "We do an inner join on the first name, the last name and the full address from the two dataframes, getting only the docs that have the same.\n",
    "\n",
    "The merge is O(M+N), if M is the number of rows in the json and N is the number of rows in the csv. \n",
    "\n",
    "Therefore, the total number of documents scanned is 2 * O(N+M) (so O(N+M)) \n",
    "\n",
    "- We scan everything for creating the DataFrames O(N+M)\n",
    "- We scan O(N+M) documents for doing the merging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of matches by name and address: 912\n"
     ]
    }
   ],
   "source": [
    "def number_of_doctor_matches_by_name_and_full_address(left_df, right_df):\n",
    "    '''Return the number of doctor matches by \n",
    "    first_name, last_name, street, street_2, city, state and zip.\n",
    "    This is done through the inner merge of two dataframes on said keys.\n",
    "    The number of rows is the number of matches'''\n",
    "    return pd.merge(left_df, \n",
    "                    right_df, \n",
    "                    how=\"inner\", \n",
    "                    on=[\"first_name\", \"last_name\", \"street\", \n",
    "                        \"street_2\", \"city\", \"state\", \"zip\"]).shape[0]\n",
    "\n",
    "\n",
    "print(\"Number of matches by name and address: {}\".format(\n",
    "    number_of_doctor_matches_by_name_and_full_address(source_unwinded_df, raw_data_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run number_of_doctor_matches_by_name_and_full_address once 0.024520256890027667\n"
     ]
    }
   ],
   "source": [
    "time_to_run_100 = timeit.timeit(\n",
    "    'number_of_doctor_matches_by_name_and_full_address(source_unwinded_df, raw_data_df)', \n",
    "    setup=\"from __main__ import number_of_doctor_matches_by_name_and_full_address, \\\n",
    "        source_unwinded_df, raw_data_df\", \n",
    "    number=100)\n",
    "\n",
    "print(\"Time to run number_of_doctor_matches_by_name_and_full_address once {}\".format(\n",
    "    time_to_run_100/100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Similarly, we need to take into account the time it took to do the transformation. However, this time to do the transformation should be shared between this and getting the number of practice matches by full address."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run transform_source_data_unwinding_fields once 0.1258921552299944\n"
     ]
    }
   ],
   "source": [
    "time_to_run_100 = timeit.timeit(\n",
    "    'transform_source_data_unwinding_fields(source_data, \\\n",
    "        fields_to_unwind=[\"practices\"], \\\n",
    "        fields_not_to_unwind=[\"doctor\"])', \n",
    "    setup=\"from __main__ import transform_source_data_unwinding_fields, source_data\", \n",
    "    number=100)\n",
    "\n",
    "print(\"Time to run transform_source_data_unwinding_fields once {}\".format(time_to_run_100/100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Match by full address\n",
    "\n",
    "Very similar to the previous case, but we do an inner join only on the full address from the two dataframes, getting only the docs that have the same full address. \n",
    "\n",
    "The number of documents scanned will be similar, O(N+M) - although a little bit less because we are doing joins in two fields less. This can also be noted in the time it takes to run one function and the other (around 5 times less for number_of_practices_by_full_address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of matches: 912\n"
     ]
    }
   ],
   "source": [
    "def number_of_practices_by_full_address(left_df, right_df):\n",
    "    '''Return the number of practices matches by \n",
    "    street, street_2, city, state and zip.\n",
    "    This is done through the inner merge of two dataframes on said keys.\n",
    "    The number of rows is the number of matches'''\n",
    "    return pd.merge(left_df, \n",
    "                    right_df, \n",
    "                    how=\"inner\", \n",
    "                    on=[\"street\", \"street_2\", \"city\", \"state\", \"zip\"]).shape[0]\n",
    "    \n",
    "print(\"Number of matches: {}\".format(\n",
    "    number_of_practices_by_full_address(source_unwinded_df, raw_data_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run number_of_practices_by_full_address once 0.019544280260015513\n"
     ]
    }
   ],
   "source": [
    "time_to_run_100 = timeit.timeit(\n",
    "    'number_of_practices_by_full_address(source_unwinded_df, raw_data_df)', \n",
    "    setup=\"from __main__ import number_of_practices_by_full_address, \\\n",
    "                                                source_unwinded_df,\\\n",
    "                                                raw_data_df\", \n",
    "    number=100)\n",
    "\n",
    "print(\"Time to run number_of_practices_by_full_address once {}\".format(\n",
    "    time_to_run_100/100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of documents that could not be matched\n",
    "\n",
    "The assumption is that the problem statement talks about the number of rows from the match_file.csv that could not be matched in source_data.json by none of the criteria above.\n",
    "\n",
    "If the doctor is matched by full name and address, the practice will match by the address (since it is a subset of the conditions).\n",
    "\n",
    "Therefore, we only need to see the intersection between the number of documents that don't match by npi and the numbers of documents that don't match by full address, this will be the number of documents that do not produce a match.\n",
    "\n",
    "For the number of documents that don't match by doctor's npi, we can do an outer merge on the npi between the dataframes from the source data and from the data to match, with an indicator that will indicate if the npi column was found on one of the two dataframes or on both.\n",
    "\n",
    "The rows of the merged table that were only in the data to match (raw_data_df) are the rows with the npis that did not match from the raw_data_df (with some extra columns and some columns named in a slightly different way due to the merge). Therefore, if we take those rows, and those columns, we will have a df that is exactly the original raw_data_df, minus the matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents not matched by npi 401\n"
     ]
    }
   ],
   "source": [
    "merged_by_npi_df = pd.merge(raw_data_df, \n",
    "                            source_not_unwinded_df, \n",
    "                            indicator=True, \n",
    "                            how=\"outer\", \n",
    "                            on=\"npi\")\n",
    "not_matched_by_npi_df = merged_by_npi_df[merged_by_npi_df['_merge'] == 'left_only']\n",
    "print(\"Number of documents not matched by npi {}\".format(not_matched_by_npi_df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first_name</th>\n",
       "      <th>last_name</th>\n",
       "      <th>npi</th>\n",
       "      <th>street</th>\n",
       "      <th>street_2</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>zip</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ruthe</td>\n",
       "      <td>Laverne</td>\n",
       "      <td>44843147983186317848</td>\n",
       "      <td>569 Glenda Islands</td>\n",
       "      <td>Suite 163</td>\n",
       "      <td>Willport</td>\n",
       "      <td>NJ</td>\n",
       "      <td>23453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Leatha</td>\n",
       "      <td>Freida</td>\n",
       "      <td>NaN</td>\n",
       "      <td>43796 Gutmann Plains</td>\n",
       "      <td>Suite 341</td>\n",
       "      <td>Vonmouth</td>\n",
       "      <td>FL</td>\n",
       "      <td>10500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Juliana</td>\n",
       "      <td>Benedict</td>\n",
       "      <td>NaN</td>\n",
       "      <td>798 Katarina Street</td>\n",
       "      <td>Apt. 817</td>\n",
       "      <td>North Florida</td>\n",
       "      <td>RI</td>\n",
       "      <td>10547-0556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Johnnie</td>\n",
       "      <td>Johnathon</td>\n",
       "      <td>NaN</td>\n",
       "      <td>541 Nora Hill</td>\n",
       "      <td>Apt. 833</td>\n",
       "      <td>South Erwinborough</td>\n",
       "      <td>UT</td>\n",
       "      <td>24212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Audra</td>\n",
       "      <td>Imogene</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9228 Rodriguez Knolls</td>\n",
       "      <td>Apt. 544</td>\n",
       "      <td>Jonesside</td>\n",
       "      <td>IN</td>\n",
       "      <td>20864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sylvia</td>\n",
       "      <td>Obie</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Victoria</td>\n",
       "      <td>Kaleb</td>\n",
       "      <td>NaN</td>\n",
       "      <td>79341 Destin Springs</td>\n",
       "      <td>Apt. 561</td>\n",
       "      <td>Port Norbertohaven</td>\n",
       "      <td>LA</td>\n",
       "      <td>74019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Erling</td>\n",
       "      <td>Ellsworth</td>\n",
       "      <td>NaN</td>\n",
       "      <td>72949 Wyman Valley</td>\n",
       "      <td>Suite 721</td>\n",
       "      <td>Shanahanton</td>\n",
       "      <td>MN</td>\n",
       "      <td>41462-4632</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   first_name  last_name                   npi                 street  \\\n",
       "0       Ruthe    Laverne  44843147983186317848     569 Glenda Islands   \n",
       "4      Leatha     Freida                   NaN   43796 Gutmann Plains   \n",
       "5     Juliana   Benedict                   NaN    798 Katarina Street   \n",
       "6     Johnnie  Johnathon                   NaN          541 Nora Hill   \n",
       "7       Audra    Imogene                   NaN  9228 Rodriguez Knolls   \n",
       "8      Sylvia       Obie                   NaN                    NaN   \n",
       "9    Victoria      Kaleb                   NaN   79341 Destin Springs   \n",
       "10     Erling  Ellsworth                   NaN     72949 Wyman Valley   \n",
       "\n",
       "     street_2                city state         zip  \n",
       "0   Suite 163            Willport    NJ       23453  \n",
       "4   Suite 341            Vonmouth    FL       10500  \n",
       "5    Apt. 817       North Florida    RI  10547-0556  \n",
       "6    Apt. 833  South Erwinborough    UT       24212  \n",
       "7    Apt. 544           Jonesside    IN       20864  \n",
       "8         NaN                 NaN   NaN         NaN  \n",
       "9    Apt. 561  Port Norbertohaven    LA       74019  \n",
       "10  Suite 721         Shanahanton    MN  41462-4632  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_columns = [\"first_name_x\", \n",
    "                    \"last_name_x\", \n",
    "                    \"npi\", \n",
    "                    \"street\",\n",
    "                    \"street_2\", \n",
    "                    \"city\", \n",
    "                    \"state\", \n",
    "                    \"zip\"]\n",
    "column_rename_map = {\"first_name_x\": \"first_name\", \"last_name_x\": \"last_name\"}\n",
    "not_matched_by_npi_df = not_matched_by_npi_df[selected_columns].rename(columns = \n",
    "                                                                       column_rename_map)\n",
    "\n",
    "not_matched_by_npi_df.head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same could be said about the number of documents that don't match by practice's full address, although the outer merge is done on the practice's full address instead of just the npi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents not matched by address 353\n"
     ]
    }
   ],
   "source": [
    "merged_by_address_df = pd.merge(raw_data_df, \n",
    "                                source_unwinded_df, \n",
    "                                indicator=True, \n",
    "                                how=\"outer\", \n",
    "                                on=[\"street\", \"street_2\", \"city\", \"state\", \"zip\"])\n",
    "not_matched_by_address_df = merged_by_address_df[merged_by_address_df['_merge'] == 'left_only']\n",
    "print(\"Number of documents not matched by address {}\".format(not_matched_by_address_df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first_name</th>\n",
       "      <th>last_name</th>\n",
       "      <th>npi</th>\n",
       "      <th>street</th>\n",
       "      <th>street_2</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>zip</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ruthe</td>\n",
       "      <td>Laverne</td>\n",
       "      <td>44843147983186317848</td>\n",
       "      <td>569 Glenda Islands</td>\n",
       "      <td>Suite 163</td>\n",
       "      <td>Willport</td>\n",
       "      <td>NJ</td>\n",
       "      <td>23453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Justyn</td>\n",
       "      <td>Abbie</td>\n",
       "      <td>78362387662864903554</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Celia</td>\n",
       "      <td>Joany</td>\n",
       "      <td>53517451823105334497</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Heather</td>\n",
       "      <td>Tracy</td>\n",
       "      <td>75216887016624818206</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sylvia</td>\n",
       "      <td>Obie</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Pascale</td>\n",
       "      <td>Ryder</td>\n",
       "      <td>75137145868784228122</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Millie</td>\n",
       "      <td>Amani</td>\n",
       "      <td>53407188811357432743</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Elvis</td>\n",
       "      <td>Lenna</td>\n",
       "      <td>14455777372842761255</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   first_name last_name                   npi              street   street_2  \\\n",
       "0       Ruthe   Laverne  44843147983186317848  569 Glenda Islands  Suite 163   \n",
       "5      Justyn     Abbie  78362387662864903554                 NaN        NaN   \n",
       "6       Celia     Joany  53517451823105334497                 NaN        NaN   \n",
       "7     Heather     Tracy  75216887016624818206                 NaN        NaN   \n",
       "8      Sylvia      Obie                   NaN                 NaN        NaN   \n",
       "9     Pascale     Ryder  75137145868784228122                 NaN        NaN   \n",
       "10     Millie     Amani  53407188811357432743                 NaN        NaN   \n",
       "11      Elvis     Lenna  14455777372842761255                 NaN        NaN   \n",
       "\n",
       "        city state    zip  \n",
       "0   Willport    NJ  23453  \n",
       "5        NaN   NaN    NaN  \n",
       "6        NaN   NaN    NaN  \n",
       "7        NaN   NaN    NaN  \n",
       "8        NaN   NaN    NaN  \n",
       "9        NaN   NaN    NaN  \n",
       "10       NaN   NaN    NaN  \n",
       "11       NaN   NaN    NaN  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_columns = [\"first_name_x\", \n",
    "                    \"last_name_x\", \n",
    "                    \"npi_x\", \n",
    "                    \"street\", \n",
    "                    \"street_2\", \n",
    "                    \"city\", \n",
    "                    \"state\", \n",
    "                    \"zip\"]\n",
    "column_rename_map = {\"first_name_x\": \"first_name\", \"last_name_x\": \"last_name\", \"npi_x\": \"npi\"}\n",
    "\n",
    "not_matched_by_address_df = not_matched_by_address_df[selected_columns].rename(columns = \n",
    "                                                                               column_rename_map)\n",
    "not_matched_by_address_df.head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we now do an inner merge on both resulting dataframes (not_matched_by_npi_df and not_matched_by_address_df) on all columns, we will have the documents that did not match in any of the cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of documents not matched 174\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first_name</th>\n",
       "      <th>last_name</th>\n",
       "      <th>npi</th>\n",
       "      <th>street</th>\n",
       "      <th>street_2</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>zip</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ruthe</td>\n",
       "      <td>Laverne</td>\n",
       "      <td>44843147983186317848</td>\n",
       "      <td>569 Glenda Islands</td>\n",
       "      <td>Suite 163</td>\n",
       "      <td>Willport</td>\n",
       "      <td>NJ</td>\n",
       "      <td>23453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sylvia</td>\n",
       "      <td>Obie</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mario</td>\n",
       "      <td>Richard</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nichole</td>\n",
       "      <td>Veda</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Steve</td>\n",
       "      <td>Fausto</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Xander</td>\n",
       "      <td>Neoma</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Marisa</td>\n",
       "      <td>Gertrude</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Madelynn</td>\n",
       "      <td>Celia</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  first_name last_name                   npi              street   street_2  \\\n",
       "0      Ruthe   Laverne  44843147983186317848  569 Glenda Islands  Suite 163   \n",
       "1     Sylvia      Obie                   NaN                 NaN        NaN   \n",
       "2      Mario   Richard                   NaN                 NaN        NaN   \n",
       "3    Nichole      Veda                   NaN                 NaN        NaN   \n",
       "4      Steve    Fausto                   NaN                 NaN        NaN   \n",
       "5     Xander     Neoma                   NaN                 NaN        NaN   \n",
       "6     Marisa  Gertrude                   NaN                 NaN        NaN   \n",
       "7   Madelynn     Celia                   NaN                 NaN        NaN   \n",
       "\n",
       "       city state    zip  \n",
       "0  Willport    NJ  23453  \n",
       "1       NaN   NaN    NaN  \n",
       "2       NaN   NaN    NaN  \n",
       "3       NaN   NaN    NaN  \n",
       "4       NaN   NaN    NaN  \n",
       "5       NaN   NaN    NaN  \n",
       "6       NaN   NaN    NaN  \n",
       "7       NaN   NaN    NaN  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents_not_matched_df = pd.merge(not_matched_by_npi_df, \n",
    "                                    not_matched_by_address_df, \n",
    "                                    how=\"inner\", \n",
    "                                    on=list(not_matched_by_npi_df.columns))\n",
    "print(\"Total number of documents not matched {}\".format(documents_not_matched_df.shape[0]))\n",
    "documents_not_matched_df.head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we make this logic into a function and then proceed to timeit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Return total number of documents not matched 174\n"
     ]
    }
   ],
   "source": [
    "def number_of_documents_not_matched_by_doctors_npi(raw_data_df, source_not_unwinded_df):\n",
    "    '''Return the number of documents not matched by doctor's npi'''\n",
    "    merged_by_npi_df = pd.merge(raw_data_df, \n",
    "                                source_not_unwinded_df, \n",
    "                                indicator=True, \n",
    "                                how=\"outer\", \n",
    "                                on=\"npi\")\n",
    "    #choose only the elements that did not match from the raw_data_df\n",
    "    not_matched_by_npi_df = merged_by_npi_df[merged_by_npi_df['_merge'] == 'left_only']\n",
    "    selected_columns_npi_df = [\"first_name_x\", \n",
    "                               \"last_name_x\",  \n",
    "                               \"npi\", \n",
    "                               \"street\", \n",
    "                               \"street_2\", \n",
    "                               \"city\", \n",
    "                               \"state\", \n",
    "                               \"zip\"]\n",
    "    column_rename_map_npi_df = {\"first_name_x\": \"first_name\", \n",
    "                                \"last_name_x\": \"last_name\"}\n",
    "    not_matched_by_npi_df = not_matched_by_npi_df[selected_columns_npi_df]\n",
    "    return not_matched_by_npi_df.rename(columns = column_rename_map_npi_df)\n",
    "\n",
    "\n",
    "def number_of_documents_not_matched_by_practices_npi(raw_data_df, source_unwinded_df):\n",
    "    '''Return the number of documents not matched by practice's full address'''\n",
    "    merged_by_address_df = pd.merge(raw_data_df, \n",
    "                                    source_unwinded_df, \n",
    "                                    indicator=True, \n",
    "                                    how=\"outer\", \n",
    "                                    on=[\"street\", \"street_2\", \"city\", \"state\", \"zip\"])\n",
    "    not_matched_by_address_df = merged_by_address_df[merged_by_address_df['_merge'] == 'left_only']\n",
    "    selected_columns_address_df = [\"first_name_x\", \n",
    "                                   \"last_name_x\", \n",
    "                                   \"npi_x\", \n",
    "                                   \"street\", \n",
    "                                   \"street_2\", \n",
    "                                   \"city\", \n",
    "                                   \"state\", \n",
    "                                   \"zip\"]\n",
    "    column_rename_map_address_df = {\"first_name_x\": \"first_name\", \n",
    "                                    \"last_name_x\": \"last_name\", \n",
    "                                    \"npi_x\": \"npi\"}\n",
    "    not_matched_by_address_df = not_matched_by_address_df[selected_columns_address_df]\n",
    "    return not_matched_by_address_df.rename(columns = column_rename_map_address_df)\n",
    "\n",
    "def number_of_documents_not_matched(raw_data_df, source_unwinded_df, source_not_unwinded_df):\n",
    "    '''Return the number of documents not matched by\n",
    "    - Doctor's npi\n",
    "    - Doctor's name and full address\n",
    "    - Practice's full adress'''\n",
    "    \n",
    "    not_matched_by_npi_df = number_of_documents_not_matched_by_doctors_npi(raw_data_df, \n",
    "                                                                           source_not_unwinded_df)\n",
    "    \n",
    "    not_matched_by_address_df = number_of_documents_not_matched_by_practices_npi(raw_data_df, \n",
    "                                                                                 source_unwinded_df)\n",
    "    \n",
    "    return pd.merge(not_matched_by_npi_df, \n",
    "                    not_matched_by_address_df, \n",
    "                    how=\"inner\", \n",
    "                    on=list(not_matched_by_npi_df.columns)).shape[0]\n",
    "\n",
    "print(\"Return total number of documents not matched {}\".format(\n",
    "    number_of_documents_not_matched(raw_data_df, source_unwinded_df, source_not_unwinded_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run number_of_documents_not_matched once 0.08916821063998213\n"
     ]
    }
   ],
   "source": [
    "time_to_run_100 = timeit.timeit(\n",
    "    'number_of_documents_not_matched(raw_data_df, source_unwinded_df, source_not_unwinded_df)', \n",
    "    setup=\"from __main__ import \\\n",
    "        number_of_documents_not_matched, raw_data_df, source_unwinded_df, source_not_unwinded_df\", \n",
    "    number=100)\n",
    "\n",
    "print(\"Time to run number_of_documents_not_matched once {}\".format(\n",
    "    time_to_run_100/100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total number of documents scanned is:\n",
    "\n",
    "- O(N+M) for the first merge + O(N+M) for getting only only the unmatched documents.\n",
    "- O(N+M) for the second merge + O(N+M) for getting only only the unmatched documents.\n",
    "- O(N+N) for the third merge (as the tables will be smaller or equal than the one in the csv)\n",
    "\n",
    "Therefore, O(N+M) documents scanned.\n",
    "\n",
    "(Remember that M is the number of rows in the json and N is the number of rows in the csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that with a small modification we could also get the total number of documents matched on doctor's npi and on practice's full address, not needing the solution we proposed before, just by doing:\n",
    "\n",
    "``` number of matches = number of rows in the csv - number of rows in the df with the documents that did not match ```\n",
    "\n",
    "With this we arrive to the final solution.\n",
    "\n",
    "## Final solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Address Match': 912,\n",
       " 'Documents Not Matched': 174,\n",
       " 'Name And Address Match': 912,\n",
       " 'Npi Match': 864}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def full_solution(match_file_path, source_file_path):\n",
    "    ''' Returns:\n",
    "    - Number of documents matched by doctor's npi\n",
    "    - Number of documents matched by doctor's name and full address\n",
    "    - Number of documents matched by practice's full adress\n",
    "    - Number of documents not matched\n",
    "    '''\n",
    "    result = {}\n",
    "    \n",
    "    # Extract and transform the source data\n",
    "    source_data = extract_source_data(source_file_path)\n",
    "    source_unwinded_df = transform_source_data_unwinding_fields(source_data, \n",
    "                                                               fields_to_unwind=[\"practices\"], \n",
    "                                                               fields_not_to_unwind=[\"doctor\"])\n",
    "    source_not_unwinded_df = transform_source_data_without_unwinding(source_data, [\"doctor\"])\n",
    "    \n",
    "    #Extract and transform the data to match\n",
    "    raw_data_df = pd.read_csv(match_file_path)\n",
    "    transform_match_file(raw_data_df, \n",
    "                         fields_to_title_case = [\"state\"], \n",
    "                         fields_to_upper_case = [\"street\", \"street_2\", \"city\"])\n",
    "\n",
    "    #Get the df with the elements of the raw_data_df not matched by doctor's npi\n",
    "    not_matched_by_npi_df = number_of_documents_not_matched_by_doctors_npi(raw_data_df, \n",
    "                                                                           source_not_unwinded_df)\n",
    "    #Get the doctor matches by doctor's npi\n",
    "    result[\"Npi Match\"] = raw_data_df.shape[0] - not_matched_by_npi_df.shape[0]\n",
    "    \n",
    "    #Get the df with the elements of the raw_data_df not matched by practice's address\n",
    "    not_matched_by_address_df = number_of_documents_not_matched_by_practices_npi(raw_data_df, \n",
    "                                                                                 source_unwinded_df)\n",
    "    #Get the practice matches by full address\n",
    "    result[\"Address Match\"] = raw_data_df.shape[0] - not_matched_by_address_df.shape[0]\n",
    "\n",
    "    #Get the matches by doctor's name and practice's address\n",
    "    result[\"Name And Address Match\"] = \\\n",
    "        number_of_doctor_matches_by_name_and_full_address(source_unwinded_df, raw_data_df)\n",
    "    \n",
    "    #Get the number of documents that weren't matched according to any criteria\n",
    "    result[\"Documents Not Matched\"] = pd.merge(not_matched_by_npi_df, \n",
    "                                               not_matched_by_address_df, \n",
    "                                               how=\"inner\", \n",
    "                                               on=list(not_matched_by_npi_df.columns)).shape[0]\n",
    "    return result\n",
    "\n",
    "\n",
    "full_solution('match_file.csv', 'source_data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run full_solution once 0.9997819791299843\n"
     ]
    }
   ],
   "source": [
    "time_to_run_100 = timeit.timeit(\n",
    "    \"full_solution('match_file.csv', 'source_data.json')\", \n",
    "    setup=\"from __main__ import full_solution\", \n",
    "    number=100)\n",
    "\n",
    "print(\"Time to run full_solution once {}\".format(time_to_run_100/100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brute force approach \n",
    "\n",
    "We can also do all of this by brute force, but it is much much slower. However, it will be left here as a sanity check. **It does not follow good coding conventions because it is not intended to be part of the final solution, just a check**.\n",
    "\n",
    "It takes about 4 minutes (245 seconds) in my machine, which compared to the second it takes to run the non-brute force approach, is 2 orders of magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Address Match': 912,\n",
       " 'Documents Not Matched': 174,\n",
       " 'Name And Address Match': 912,\n",
       " 'Npi Match': 864}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def check_row_for_matches(df_row, result, source_data):\n",
    "    '''Check if dataframe row (series) has any match in the source data'''\n",
    "    for record in source_data:\n",
    "        npi_match = False\n",
    "        practice_match = False\n",
    "        if record[\"doctor\"][\"npi\"] == df_row[\"npi\"]:\n",
    "            result[\"Npi Match\"] += 1\n",
    "            npi_match = True\n",
    "        for practice in record[\"practices\"]:\n",
    "            full_address = [\"street\", \"street_2\", \"city\", \"state\", \"zip\"]\n",
    "            if all(practice[x] == df_row[x] for x in full_address):\n",
    "                result[\"Address Match\"] += 1\n",
    "                practice_match = True\n",
    "                if record[\"doctor\"][\"first_name\"] == df_row[\"first_name\"] and \\\n",
    "                    record[\"doctor\"][\"first_name\"] == df_row[\"first_name\"]:\n",
    "                        result[\"Name And Address Match\"] += 1\n",
    "                break\n",
    "        if npi_match or practice_match:\n",
    "            result[\"Documents Not Matched\"] -= 1\n",
    "            break\n",
    "    return result\n",
    "\n",
    "def brute_force_approach(match_file_path, source_file_path):    \n",
    "    # Extract the source data\n",
    "    source_data = extract_source_data(source_file_path)\n",
    "    \n",
    "    #Extract and transform the data to match\n",
    "    raw_data_df = pd.read_csv(match_file_path)\n",
    "    transform_match_file(raw_data_df, \n",
    "                         fields_to_title_case = [\"state\"], \n",
    "                         fields_to_upper_case = [\"street\", \"street_2\", \"city\"])\n",
    "\n",
    "    result = {\n",
    "        \"Npi Match\": 0,\n",
    "        \"Address Match\": 0,\n",
    "        \"Name And Address Match\": 0,\n",
    "        \"Documents Not Matched\": raw_data_df.shape[0]\n",
    "    }\n",
    "    \n",
    "    raw_data_df.apply(check_row_for_matches, args=(result, source_data), axis = 1)\n",
    "    return result\n",
    "\n",
    "brute_force_approach(\"match_file.csv\", \"source_data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run brute_force_approach once 246.68699994200142\n"
     ]
    }
   ],
   "source": [
    "time_to_run_1 = timeit.timeit(\n",
    "    'brute_force_approach(\"match_file.csv\", \"source_data.json\")', \n",
    "    setup=\"from __main__ import brute_force_approach\", \n",
    "    number=1)\n",
    "\n",
    "print(\"Time to run brute_force_approach once {}\".format(time_to_run_1))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
